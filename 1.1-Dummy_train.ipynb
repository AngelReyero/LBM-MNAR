{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy training version "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0- Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy.special import logit\n",
    "from fcts.lbm_nmar import LBM_NMAR\n",
    "from fcts.lbfgs import FullBatchLBFGS\n",
    "from fcts.utils import reparametrized_expanded_params, inv_softplus, shrink_simplex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Load torch parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (for Mac) %env PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0\n",
    "device = 'mps' #put 'cuda', 'cpu' or 'mps' (for Mac)\n",
    "device2 = 'mps' #put None, 'cuda' or 'mps' (for Mac)\n",
    "\n",
    "if not torch.backends.mps.is_available() and device != 'cpu':\n",
    "    print('Cuda is not available. Algorithm will use cpu')\n",
    "    device, device2 = torch.device('cpu'), None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Load parliament datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#votes: matrix gathering votes for several laws and politicians (1: positive, 0: missing/abstention, -1: negative)\n",
    "votes = np.loadtxt(\"data_parliament/votes.txt\",delimiter=\";\").astype(int)\n",
    "\n",
    "#deputes: Family name, Name, Political group \n",
    "deputes = json.load(open('data_parliament/deputes.json', 'r')) \n",
    "\n",
    "#texts:  political group demanding the law, title of demand, date, type (type of vote, type of majority, name of type of vote), \n",
    "texts = json.load(open('data_parliament/texts.json', 'r')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Train one iteration by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a - Parameter initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row length (number of MPs):  576\n",
      "col length (number of ballots):  1256\n"
     ]
    }
   ],
   "source": [
    "n1, n2 = votes.shape\n",
    "print(\"row length (number of MPs): \",n1)\n",
    "print(\"col length (number of ballots): \",n2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a number of row and column clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select number of row clusters\n",
    "nq = 3# COMPLETE\n",
    "\n",
    "# Select number of column clusters\n",
    "nl = 5 # COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of the variational parameters: \n",
    "\n",
    "$\\gamma = (\\nu_a, \\nu_b, \\nu_p, \\nu_q, \\rho_a, \\rho_b,\\rho_p,\\rho_q, \\tau_1, \\tau_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ν: mean of restrincted variational distribution q_{gamma}\n",
    "nu_a = np.random.uniform(-0.5, 0.5, (n1, 1))# column vector of size n1\n",
    "nu_b = np.random.uniform(-0.5, 0.5, (n1, 1))\n",
    "nu_p = np.random.uniform(-0.5, 0.5, (1, n2)) # row vector of size n2\n",
    "nu_q = np.random.uniform(-0.5, 0.5, (1, n2))\n",
    "\n",
    "# ρ: mean of restrincted variational distribution q_{gamma}  \n",
    "rho_a = 1e-5 * np.ones((n1, 1))# column vector of size n1\n",
    "rho_b = 1e-5 * np.ones((n1, 1))\n",
    "rho_p = 1e-5 * np.ones((1, n2)) # row vector of size n2\n",
    "rho_q = 1e-5 * np.ones((1, n2))\n",
    "\n",
    "# τ: probability of each row (resp. column), to be in cluster k ∈ {1,...,nq} (resp. l ∈ {1,...,nl})\n",
    "tau_1 = np.diff(\n",
    "        np.concatenate(\n",
    "            (np.zeros((n1, 1)),\n",
    "             np.sort(np.random.uniform(size=(n1, nq - 1)), axis=1),\n",
    "             np.ones((n1, 1)),),axis=1,),\n",
    "        axis=1,\n",
    "    ) #size (n1,nq)\n",
    "\n",
    "tau_2 = np.diff(\n",
    "        np.concatenate(\n",
    "            (np.zeros((n2, 1)),\n",
    "            np.sort(np.random.uniform(size=(n2, nl - 1)), axis=1),\n",
    "            np.ones((n2, 1)),\n",
    "            ),\n",
    "            axis=1,\n",
    "        ),\n",
    "        axis=1,\n",
    "    ) #size (n2, nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = np.concatenate((\n",
    "            nu_a.flatten(),\n",
    "            inv_softplus(rho_a.flatten()),\n",
    "            nu_b.flatten(),\n",
    "            inv_softplus(rho_b.flatten()),\n",
    "            nu_p.flatten(),\n",
    "            inv_softplus(rho_p.flatten()),\n",
    "            nu_q.flatten(),\n",
    "            inv_softplus(rho_q.flatten()),\n",
    "            logit(shrink_simplex(tau_1).flatten()),\n",
    "            logit(shrink_simplex(tau_2).flatten()),\n",
    "        ))\n",
    "\n",
    "# Just-in-case checks: \n",
    "assert len(gamma.shape) == 1\n",
    "assert gamma.shape[0] == 4 * n1 + 4 * n2 + (n1 * (nq - 1)) + (n2 * (nl - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of model parameters: \n",
    "\n",
    "$\\theta = (\\mu , \\sigma_a^2, \\sigma_b^2, \\sigma_p^2, \\sigma_q^2, \\alpha_1, \\alpha_2, \\pi)$, where $\\mu$ and $\\sigma_i for i \\in {a,b,p,q}$ refer to missingness mechanism variables and $\\pi$ and $\\alpha_1, \\alpha_2$ refer to LBM variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# μ: where expit(μ) is the global missingness rate\n",
    "mu_un = np.random.uniform(-4.5, -3.5) \n",
    "\n",
    "# σ^2: variances of latent variables A, B, C, D\n",
    "sigma_sq_a = np.random.uniform(0.4, 0.7) \n",
    "sigma_sq_b = np.random.uniform(0.4, 0.7)\n",
    "sigma_sq_p = np.random.uniform(0.4, 0.7)\n",
    "sigma_sq_q = np.random.uniform(0.4, 0.7)\n",
    "\n",
    "# α_1k (resp. α_2k): probability of being in row (resp. column) cluster k \n",
    "alpha_1 = (np.ones(nq) / nq).reshape((nq, 1)) #uniform proba of each row cluster\n",
    "alpha_2 = (np.ones(nl) / nl).reshape((1, nl)) #idem for col cluster\n",
    "\n",
    "# π_{k,l}: probability of being in cluster k & l\n",
    "pi = np.random.uniform(0.2, 0.8, (nq, nl)) #pi_kl (size: nb row clust  nb col clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.concatenate((\n",
    "            (mu_un,),\n",
    "            (inv_softplus(sigma_sq_a),),\n",
    "            (inv_softplus(sigma_sq_b),),\n",
    "            (inv_softplus(sigma_sq_p),),\n",
    "            (inv_softplus(sigma_sq_q),),\n",
    "            logit(shrink_simplex(alpha_1.T).flatten()),\n",
    "            logit(shrink_simplex(alpha_2).flatten()),\n",
    "            logit(pi.flatten()),\n",
    "        ))\n",
    "\n",
    "# Just-in-case checks:\n",
    "assert len(theta.shape) == 1\n",
    "assert theta.shape[0] == 5 + nq - 1 + nl - 1 + nq * nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_of_parameters = torch.tensor(np.concatenate((gamma, theta)), requires_grad=True, device=device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loglike_dist_tol=1e-4\n",
    "max_iter=100\n",
    "norm_grad_tol=1e-4\n",
    "initial_learning_rate=1.0\n",
    "hessian_history_size=100\n",
    "loglike_diff_breaking_cond=1e-3\n",
    "divide_by_line_search=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b- Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LBM_NMAR(vector_of_parameters,votes,(n1, n2, nq, nl),device=device,device2=device2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(For more details check the file: 1.2-Model_LBM_MNAR.ipynb)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c- Train model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Variational EM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VE step:** Find the variational parameter $\\gamma^{(t+1)}$ \n",
    "\n",
    "$\\gamma^{(t+1)} = argmax  J(\\gamma, \\theta^t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------- \n",
      "Start training LBM MNAR \n",
      " --------------------------------------------------------------------------------\n",
      "VE step\n",
      "  LBFGS iter  | criteria |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurafuentesvicente/M2 Maths&IA/Methodes Non Supervises avancees/Projet/LBM-MNAR/fcts/lbfgs.py:339: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1519.)\n",
      "  p.data.add_(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1  | 340395.75000 |\n",
      " 2  | 324425.09375 |\n",
      " 3  | 319082.78125 |\n",
      " 4  | 316506.81250 |\n",
      " 5  | 315350.34375 |\n",
      " 6  | 314795.50000 |\n",
      " 7  | 314347.12500 |\n",
      " 8  | 314017.68750 |\n",
      " 9  | 313716.06250 |\n",
      " 10  | 313385.00000 |\n",
      " 11  | 313036.28125 |\n",
      " 12  | 312629.21875 |\n",
      " 13  | 312103.50000 |\n",
      " 14  | 311624.21875 |\n",
      " 15  | 311094.87500 |\n",
      " 16  | 310443.87500 |\n",
      " 17  | 310193.15625 |\n",
      " 18  | 310092.87500 |\n",
      " 19  | 310048.15625 |\n",
      " 20  | 310009.25000 |\n",
      " 21  | 310007.62500 |\n",
      " 22  | 310007.03125 |\n",
      " 23  | 310007.03125 |\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 80, \"\\nStart training LBM MNAR\", \"\\n\", \"-\" * 80)\n",
    "print(\"VE step\")\n",
    "print(f\"\"\"  LBFGS iter  | criteria |\"\"\")\n",
    "\n",
    "# Declaration of optimization variables for gamma optimization \n",
    "line_search = \"Armijo\"\n",
    "optimizer = FullBatchLBFGS([model.variationnal_params],lr=initial_learning_rate,history_size=hessian_history_size,line_search=line_search,debug=True)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "obj = model()\n",
    "obj.backward()\n",
    "f_old = obj.item() # update with current iteration value \n",
    "\n",
    "for n_iter in range(max_iter):\n",
    "    # optimize the gamma params (a maximum of iteration)\n",
    "    def closure():\n",
    "        # define closure for line search\n",
    "        loss_fn = model(no_grad=True) #definition of gradient here \n",
    "        return loss_fn\n",
    "    \n",
    "    ### Perform line search step\n",
    "    options = {\"closure\": closure,\"current_loss\": obj,\"eta\": divide_by_line_search,\"max_ls\": 150,\"interpolate\": False,\"inplace\": True,\"ls_debug\": False,\"damping\": False,\"eps\": 1e-2,\"c1\": 0.5,\"c2\": 0.95,}\n",
    "                    \n",
    "    # Optimization part \n",
    "    obj, lr, backtracks, clos_evals, desc_dir, fail = optimizer.step(options=options)  \n",
    "    optimizer.zero_grad() # put gradients to 0\n",
    "    obj = model() #call function to optimize\n",
    "    obj.backward() # compute optimization\n",
    "    grad = optimizer._gather_flat_grad()\n",
    "\n",
    "\n",
    "    print(f\"\"\" {n_iter + 1}  | {obj.item():.5f} |\"\"\")\n",
    "    if (torch.norm(grad) < norm_grad_tol or np.abs(obj.item() - f_old) < loglike_dist_tol):\n",
    "        break\n",
    "    f_old = obj.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**M step:** Find the model parameter $\\theta^{(t+1)}$ \n",
    "\n",
    "$\\theta^{(t+1)} = argmax  J(\\gamma^{(t+1)}, \\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M step\n",
      "  LBFGS iter  | criteria |\n",
      " 1  | 309633.21875 |\n",
      " 2  | 308944.75000 |\n",
      " 3  | 307853.93750 |\n",
      " 4  | 307616.68750 |\n",
      " 5  | 306878.90625 |\n",
      " 6  | 306739.46875 |\n",
      " 7  | 306662.84375 |\n",
      " 8  | 306629.78125 |\n",
      " 9  | 306610.31250 |\n",
      " 10  | 306590.09375 |\n",
      " 11  | 306575.15625 |\n",
      " 12  | 306559.56250 |\n",
      " 13  | 306550.65625 |\n",
      " 14  | 306545.03125 |\n",
      " 15  | 306542.18750 |\n",
      " 16  | 306537.78125 |\n",
      " 17  | 306536.03125 |\n",
      " 18  | 306534.96875 |\n",
      " 19  | 306534.68750 |\n",
      " 20  | 306534.53125 |\n",
      " 21  | 306534.43750 |\n",
      " 22  | 306534.37500 |\n",
      " 23  | 306534.34375 |\n",
      " 24  | 306534.34375 |\n"
     ]
    }
   ],
   "source": [
    "print(\"M step\")\n",
    "print(f\"\"\"  LBFGS iter  | criteria |\"\"\")\n",
    "\n",
    "# Declaration of optimization variables for theta optimization \n",
    "optimizer = FullBatchLBFGS([model.model_params],lr=initial_learning_rate,history_size=hessian_history_size,line_search=line_search,debug=True)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "obj = model()\n",
    "obj.backward()\n",
    "f_old = obj.item()\n",
    "\n",
    "\n",
    "for n_iter in range(max_iter):\n",
    "    # optimize the relative params (a maximum of iteration)\n",
    "    # define closure for line search\n",
    "    def closure():\n",
    "        loss_fn = model(no_grad=True) #definition of gradient here \n",
    "        return loss_fn\n",
    "    \n",
    "    ### perform line search step\n",
    "    options = {\"closure\": closure,\"current_loss\": obj,\"eta\": divide_by_line_search,\"max_ls\": 150,\"interpolate\": False,\"inplace\": True,\"ls_debug\": False,\"damping\": False,\"eps\": 1e-2,\"c1\": 0.5,\"c2\": 0.95,}\n",
    "                    \n",
    "    # Optimization part \n",
    "    obj, lr, backtracks, clos_evals, desc_dir, fail = optimizer.step(options=options)  \n",
    "    optimizer.zero_grad() # put gradients to 0\n",
    "    obj = model() #call function to optimize\n",
    "    obj.backward() # compute optimization\n",
    "    grad = optimizer._gather_flat_grad()\n",
    "\n",
    "\n",
    "    print(f\"\"\" {n_iter + 1}  | {obj.item():.5f} |\"\"\")\n",
    "    if (torch.norm(grad) < norm_grad_tol or np.abs(obj.item() - f_old) < loglike_dist_tol):\n",
    "        break\n",
    "    f_old = obj.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reparametrization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "(   nu_a,\n",
    "    rho_a,\n",
    "    nu_b,\n",
    "    rho_b,\n",
    "    nu_p,\n",
    "    rho_p,\n",
    "    nu_q,\n",
    "    rho_q,\n",
    "    tau_1,\n",
    "    tau_2,\n",
    "    mu_un,\n",
    "    sigma_sq_a,\n",
    "    sigma_sq_b,\n",
    "    sigma_sq_p,\n",
    "    sigma_sq_q,\n",
    "    alpha_1,\n",
    "    alpha_2,\n",
    "    pi,\n",
    "    ) = reparametrized_expanded_params(torch.cat((model.variationnal_params, model.model_params)), n1, n2, nq, nl, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
