{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focus on LBM MNAR model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0- Library importation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from fcts.utils import reparametrized_expanded_params, d2_DL3_XO, init_random_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Load dataset & parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a- Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#votes: matrix gathering votes for several laws and politicians (1: positive, 0: missing/abstention, -1: negative)\n",
    "votes = np.loadtxt(\"data_parliament/votes.txt\",delimiter=\";\").astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b- Parameter initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset size\n",
    "n1, n2 = votes.shape\n",
    "nq = 3 # COMPLETE\n",
    "nl = 5 #COMPLETE\n",
    "\n",
    "#torch parameters\n",
    "device = 'mps' #put 'cuda', 'cpu' or 'mps' (for Mac)\n",
    "device2 = 'mps' #put None, 'cuda' or 'mps' (for Mac) \n",
    "\n",
    "# gamma and theta \n",
    "vector_of_parameters = torch.tensor(init_random_params(n1, n2, nq, nl), requires_grad=True, device=device, dtype=torch.float32)\n",
    "lengamma = (4 * n1+ 4 * n2+ (n1 * (nq - 1))+ (n2 * (nl - 1)))\n",
    "variationnal_params = vector_of_parameters[:lengamma].clone()\n",
    "model_params = vector_of_parameters[lengamma:].clone()\n",
    "\n",
    "#indices\n",
    "indices_p = np.argwhere(votes == 1) #argwhere: matrix with couples (row,column) with 1 values \n",
    "indices_n = np.argwhere(votes == -1) #idem with -1\n",
    "indices_zeros = np.argwhere(votes == 0) #idem with 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Computation of the criterion: \n",
    "\n",
    "$J(\\gamma, \\theta) = H(q) + E_q[log(p(X^o, Y_1, Y_2, A, B, C, D, \\gamma ))]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a- Entropy \n",
    "\n",
    "$H(q) = - \\Sigma_{ik} \\tau_{ik}^{(Y_1)}log(\\tau_{ik}^{(Y_1)}) - \\Sigma_{jl} \\tau_{jl}^{(Y_2)}log(\\tau_{jl}^{(Y_2)}) + \\frac{1}{2} \\Sigma_i log(2\\pi e \\rho_i^{(A)}) + \\frac{1}{2} \\Sigma_i log(2\\pi e \\rho_i^{(B)})+ \\frac{1}{2} \\Sigma_j log(2\\pi e \\rho_j^{(C)})+ \\frac{1}{2} \\Sigma_j log(2\\pi e \\rho_j^{(D)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_rx(rho_a, rho_b, rho_p, rho_q, tau_1, tau_2):\n",
    "    return (1/2* (2 * n1 + 2 * n2)* \n",
    "        (torch.log(torch.tensor(2 * np.pi, dtype=torch.float32, device=device))+ 1)\n",
    "        + 1 / 2 * torch.sum(torch.log(rho_a))\n",
    "        + 1 / 2 * torch.sum(torch.log(rho_b))\n",
    "        + 1 / 2 * torch.sum(torch.log(rho_p))\n",
    "        + 1 / 2 * torch.sum(torch.log(rho_q))\n",
    "        - torch.sum(tau_1 * torch.log(tau_1))\n",
    "        - torch.sum(tau_2 * torch.log(tau_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b- Expectation with respect to the variational distribution \n",
    "\n",
    "$E_q[log(p(X^o, Y_1, Y_2, A, B, C, D, \\gamma ))] = E_{q_{\\gamma}}[log(p(Y_1))] + E_{q_{\\gamma}}[log(p(Y_2))] + E_{q_{\\gamma}}[log(p(A))] + E_{q_{\\gamma}}[log(p(B))] + E_{q_{\\gamma}}[log(p(C))] + E_{q_{\\gamma}}[log(p(D))]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E_{q_{\\gamma}}[log(p(Y_1))] = \\Sigma_{ik} \\tau_{ik}^{(Y_1)}log(\\alpha_k)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectation_loglike_Y1(tau_1, alpha_1):\n",
    "    return tau_1.sum(0) @ torch.log(alpha_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E_{q_{\\gamma}}[log(p(Y_2))] = \\Sigma_{jl} \\tau_{jl}^{(Y_2)}log(\\beta_l)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectation_loglike_Y2(tau_2, alpha_2):\n",
    "    return tau_2.sum(0) @ torch.log(alpha_2).t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E_{q_{\\gamma}}[log(p(I))] = -\\frac{n_1}{2}log(2\\pi) - \\frac{n_1}{2}log(\\sigma_I^2) - \\frac{1}{2\\sigma_I^2} \\Sigma_{i} (((\\nu_i^{(I)})^2 + \\rho_i^{(I)}))$ for $I \\in {A, B}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectation_loglike_A(nu_a, rho_a, sigma_sq_a):\n",
    "        return -n1 / 2 * (torch.log(torch.tensor(2 * np.pi, dtype=torch.float32, device=device))+ torch.log(sigma_sq_a)) - 1 / (2 * sigma_sq_a) * torch.sum(rho_a + nu_a ** 2)\n",
    "\n",
    "def expectation_loglike_B(nu_b, rho_b, sigma_sq_b):\n",
    "    return -n1 / 2 * (torch.log(torch.tensor(2 * np.pi, dtype=torch.float32, device=device))+ torch.log(sigma_sq_b)) - 1 / (2 * sigma_sq_b) * torch.sum(rho_b + nu_b ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E_{q_{\\gamma}}[log(p(J))] = -\\frac{n_2}{2}log(2\\pi) - \\frac{n_2}{2}log(\\sigma_J^2) - \\frac{1}{2\\sigma_J^2} \\Sigma_{j} (((\\nu_j^{(J)})^2 + \\rho_j^{(J)}))$ for $J \\in {C, D}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectation_loglike_P(nu_p, rho_p, sigma_sq_p):\n",
    "    return -n2 / 2 * (torch.log(torch.tensor(2 * np.pi, dtype=torch.float32, device=device))+ torch.log(sigma_sq_p)) - 1 / (2 * sigma_sq_p) * torch.sum(rho_p + nu_p ** 2)\n",
    "\n",
    "def expectation_loglike_Q(nu_q, rho_q, sigma_sq_q):\n",
    "    return -n2 / 2 * (torch.log(torch.tensor(2 * np.pi, dtype=torch.float32, device=device))+ torch.log(sigma_sq_q)) - 1 / (2 * sigma_sq_q) * torch.sum(rho_q + nu_q ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E_{q_\\gamma}[log(p(X^o|Y_1, Y_2, A, B, C, D))] = \\Sigma_{(kl, ij, X_{ij}^o=1)}\\tau_{ij}^{(Y_1)}\\tau_{jl}^{(Y_2)}E_{q_{\\gamma}}[log(p_1)] + \\Sigma_{(kl, ij, X_{ij}^o=0)}\\tau_{ij}^{(Y_1)}\\tau_{jl}^{(Y_2)}E_{q_{\\gamma}}[log(p_0)] + \\Sigma_{(kl, ij, X_{ij}^o=NA)}\\tau_{ij}^{(Y_1)}\\tau_{jl}^{(Y_2)}E_{q_{\\gamma}}[log(1- p_0 -p_1)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where: \n",
    "\n",
    " $E_{q_{\\gamma}}[f_h(A_i + C_j, B_i + D_j)] â‰ˆ f_h(\\nu_i^{(A)} + \\nu_j^{(C)}, \\nu_i^{(B)} + \\nu_j^{(D)}) + \\frac{1}{2}(\\rho_i^{(A)} + \\rho_j^{(C)})\\frac{\\partial^2f_h(\\nu_i^{(A)} + \\nu_j^{(C)}, \\nu_i^{(B)} + \\nu_j^{(D)})}{\\partial(\\nu_i^{(A)} + \\nu_j^{(C)})^2} + \\frac{1}{2}(\\rho_i^{(B)} + \\rho_j^{(D)})\\frac{\\partial^2f_h(\\nu_i^{(A)} + \\nu_j^{(C)}, \\nu_i^{(B)} + \\nu_j^{(D)})}{\\partial(\\nu_i^{(B)} + \\nu_j^{(B)})^2} $ for $h \\in \\{1, 0, NA\\}$\n",
    " \n",
    " \n",
    " and \n",
    " \n",
    " $f_1(x,y) = log(\\pi_{kl}expit(\\mu + x + y))$\n",
    "\n",
    "$f_0(x,y) = log((1 - \\pi_{kl})expit(\\mu + x - y))$\n",
    "\n",
    "$f_{NA}(x,y) = log(1 - \\pi_{kl}expit(\\mu + x +y) - (1 - \\pi_{kl})expit(\\mu + x -y))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectation_loglike_X_cond_ABPY1Y2(\n",
    "        mu_un,nu_a,nu_b,nu_p,nu_q,rho_a,rho_b,rho_p,rho_q,tau_1,tau_2,pi):\n",
    "        i_p_one = indices_p[:, 0] # takes rows Xij=1\n",
    "        i_m_one = indices_n[:, 0] #takes rows Xij=0\n",
    "        i_zeros = indices_zeros[:, 0] #takes rows Xij=NA\n",
    "        j_p_one = indices_p[:, 1] # cols Xij=1\n",
    "        j_m_one = indices_n[:, 1] #cols Xij=0\n",
    "        j_zeros = indices_zeros[:, 1] #cols Xij=NA\n",
    "\n",
    "        ## Positives: Xij= 1 ### \n",
    "        xp = nu_a[i_p_one].flatten() + nu_p[:, j_p_one].flatten() #for A & C\n",
    "        yp = nu_b[i_p_one].flatten() + nu_q[:, j_p_one].flatten() #for B & D\n",
    "\n",
    "        sig_p = torch.sigmoid(mu_un+ nu_a[i_p_one]+ nu_b[i_p_one]+ nu_p[:, j_p_one].t()+ nu_q[:, j_p_one].t())\n",
    "        der2_sig_p = (-sig_p * (1 - sig_p)).flatten()\n",
    "        sum_var_p = (rho_a[i_p_one].flatten()+ rho_p[:, j_p_one].flatten()+ rho_b[i_p_one].flatten()+ rho_q[:, j_p_one].flatten())\n",
    "\n",
    "        # f_{1}\n",
    "        f = lambda x, y: torch.log(pi.view(1, nq, nl) * torch.sigmoid(mu_un + x + y).view(-1, 1, 1))\n",
    "        \n",
    "        # Taylor development (1)\n",
    "        expectation_taylor_p = (tau_1[i_p_one].view(-1, nq, 1)\n",
    "            * tau_2[j_p_one].view(-1, 1, nl)\n",
    "            * (f(xp, yp) + 0.5 * (der2_sig_p * sum_var_p).view(-1, 1, 1))).sum()\n",
    "\n",
    "        ### Negatives: Xij= -1 ###\n",
    "        xn = nu_a[i_m_one].flatten() + nu_p[:, j_m_one].flatten()\n",
    "        yn = nu_b[i_m_one].flatten() + nu_q[:, j_m_one].flatten()\n",
    "        \n",
    "        sig_m = torch.sigmoid(mu_un+ nu_a[i_m_one]- nu_b[i_m_one]+ nu_p[:, j_m_one].t()- nu_q[:, j_m_one].t())\n",
    "        der2_sig_m = -(sig_m * (1 - sig_m)).flatten()\n",
    "        sum_var_m = (rho_a[i_m_one].flatten()+ rho_p[:, j_m_one].flatten()+ rho_b[i_m_one].flatten()+ rho_q[:, j_m_one].flatten())\n",
    "        \n",
    "        #f_{0}\n",
    "        f = lambda x, y: torch.log((1 - pi).view(1, nq, nl)* torch.sigmoid(mu_un + x - y).view(-1, 1, 1))\n",
    "\n",
    "        # Taylor development (0)\n",
    "        expectation_taylor_m = (tau_1[i_m_one].view(-1, nq, 1)* tau_2[j_m_one].view(-1, 1, nl)\n",
    "            * (f(xn, yn) + 0.5 * (der2_sig_m * sum_var_m).view(-1, 1, 1))).sum()\n",
    "\n",
    "        ### Zeros: Xij= 0 ###\n",
    "\n",
    "        # f_{NA}\n",
    "        f = lambda x, y: torch.log(1- pi.view(1, nq, nl) * torch.sigmoid(mu_un + x + y).view(-1, 1, 1)- (1 - pi.view(1, nq, nl))* torch.sigmoid(mu_un + x - y).view(-1, 1, 1))\n",
    "        xz = nu_a[i_zeros].flatten() + nu_p[:, j_zeros].flatten()\n",
    "        yz = nu_b[i_zeros].flatten() + nu_q[:, j_zeros].flatten()\n",
    "\n",
    "        if device2:\n",
    "            der_x = d2_DL3_XO(xz.view(-1, 1, 1).to(device2),yz.view(-1, 1, 1).to(device2),mu_un.to(device2),pi.view(1, nq, nl).to(device2),\"x\",).to(device)\n",
    "        else:\n",
    "            der_x = d2_DL3_XO(xz.view(-1, 1, 1),yz.view(-1, 1, 1),mu_un,pi.view(1, nq, nl),\"x\",)\n",
    "\n",
    "        der_y = d2_DL3_XO(xz.view(-1, 1, 1),yz.view(-1, 1, 1),mu_un,pi.view(1, nq, nl),\"y\",)\n",
    "\n",
    "        tau_12_ij = tau_1[i_zeros].view(-1, nq, 1) * tau_2[j_zeros].view(-1, 1, nl)\n",
    "        \n",
    "        # Taylor development (NA)\n",
    "        expectation_taylor_zeros = (tau_12_ij* (f(xz, yz)+ 0.5\n",
    "                * (der_x* (rho_a[i_zeros].flatten() + rho_p[:, j_zeros].flatten()).view(-1, 1, 1)\n",
    "                    + der_y* (rho_b[i_zeros].flatten() + rho_q[:, j_zeros].flatten()).view(-1, 1, 1)))).sum()\n",
    "\n",
    "\n",
    "\n",
    "        ### Final expectation ###\n",
    "        expectation = (expectation_taylor_p+ expectation_taylor_m+ expectation_taylor_zeros)\n",
    "        \n",
    "        return (expectation if expectation < 0 else torch.tensor(np.inf, device=device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final computation of criterion: \n",
    "\n",
    "$J(\\gamma, \\theta) = H(q) + E_q[log(p(X^o, Y_1, Y_2, A, B, C, D, \\gamma ))]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criteria(variationnal_params, model_params):\n",
    "        (nu_a,rho_a,nu_b,rho_b,nu_p,rho_p,nu_q,rho_q,tau_1,tau_2,mu_un,sigma_sq_a,sigma_sq_b,sigma_sq_p,sigma_sq_q,alpha_1,alpha_2,\n",
    "            pi) = reparametrized_expanded_params(torch.cat((variationnal_params, model_params)),n1,n2,nq,nl,device,)\n",
    "        if torch.any(tau_1.sum(dim=0) < 0.5):\n",
    "            print(\"One empty row class, algo stoped\")\n",
    "            return torch.tensor(np.nan, device=device)\n",
    "        if torch.any(tau_2.sum(dim=0) < 0.5):\n",
    "            print(\"One empty column class, algo stoped\")\n",
    "            return torch.tensor(np.nan, device=device)\n",
    "        \n",
    "        \n",
    "        expectation = (entropy_rx(rho_a, rho_b, rho_p, rho_q, tau_1, tau_2)\n",
    "            + expectation_loglike_A(nu_a, rho_a, sigma_sq_a)[0]\n",
    "            + expectation_loglike_B(nu_b, rho_b, sigma_sq_b)[0]\n",
    "            + expectation_loglike_P(nu_p, rho_p, sigma_sq_p)[0]\n",
    "            + expectation_loglike_Q(nu_q, rho_q, sigma_sq_q)[0]\n",
    "            + expectation_loglike_Y1(tau_1, alpha_1)\n",
    "            + expectation_loglike_Y2(tau_2, alpha_2)\n",
    "            + expectation_loglike_X_cond_ABPY1Y2(mu_un,nu_a,nu_b,nu_p,nu_q,rho_a,rho_b,rho_p,rho_q,tau_1,tau_2,pi)\n",
    "        )\n",
    "        \n",
    "        #We return -expectation to transform the maximization problem into a minimization problem \n",
    "        return -expectation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
