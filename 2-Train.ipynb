{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the model for parliament dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0- Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from fcts.train_procedure import train_with_LBFGS\n",
    "from fcts.lbm_nmar import LBM_NMAR\n",
    "from fcts.utils import reparametrized_expanded_params, init_random_params, save_objects_to_yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Load torch parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (for Mac) %env PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0\n",
    "device = 'cuda' #put 'cuda', 'cpu' or 'mps' (for Mac)\n",
    "device2 = 'cuda' #put None, 'cuda' or 'mps' (for Mac)\n",
    "\n",
    "if not torch.backends.mps.is_available() and device != 'cpu':\n",
    "    print('Cuda is not available. Algorithm will use cpu')\n",
    "    device, device2 = torch.device('cpu'), None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Load parliament datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#votes: matrix gathering votes for several laws and politicians (1: positive, 0: missing/abstention, -1: negative)\n",
    "votes = np.loadtxt(\"data_parliament/votes.txt\",delimiter=\";\").astype(int)\n",
    "\n",
    "#deputes: Family name, Name, Political group \n",
    "deputes = json.load(open('data_parliament/deputes.json', 'r')) \n",
    "\n",
    "#texts:  political group demanding the law, title of demand, date, type (type of vote, type of majority, name of type of vote), \n",
    "texts = json.load(open('data_parliament/texts.json', 'r')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indices\n",
    "indices_p = np.argwhere(votes == 1) #argwhere: matrix with couples (row,column) with 1 values \n",
    "indices_n = np.argwhere(votes == -1) #idem with -1\n",
    "indices_zeros = np.argwhere(votes == 0) #idem with 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a - Parameter initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shape of votes dataset: number of rows and columns\n",
    "n1, n2 = votes.shape\n",
    "\n",
    "# Select number of row clusters\n",
    "nq = 3# COMPLETE\n",
    "\n",
    "# Select number of column clusters\n",
    "nl = 5 # COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of ($\\gamma, \\theta$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_of_parameters = torch.tensor(init_random_params(n1, n2, nq, nl), requires_grad=True, device=device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b- Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LBM_NMAR(\n",
    "    vector_of_parameters,\n",
    "    votes,\n",
    "    (n1, n2, nq, nl),\n",
    "    device=device,\n",
    "    device2=device2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c- Train model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform variational EM: \n",
    "\n",
    "VEM step % 2 = 0: VE step, where we maximize the variational parameters $\\gamma$\n",
    "\n",
    "\n",
    "VEM step % 2 = 1: M step, where we maximize the model parameters $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------- \n",
      "Start training LBM MNAR \n",
      " --------------------------------------------------------------------------------\n",
      "Number of row classes :  3\n",
      "Number of col classes :  5\n",
      " VEM step  |   LBFGS iter  | criteria |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurafuentesvicente/M2 Maths&IA/Methodes Non Supervises avancees/Projet/LBM-MNAR/fcts/lbfgs.py:339: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1519.)\n",
      "  p.data.add_(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0  |   1  | 337169.15625 |\n",
      " 0  |   2  | 322914.09375 |\n",
      " 0  |   3  | 317270.81250 |\n",
      " 0  |   4  | 315133.43750 |\n",
      " 0  |   5  | 314240.21875 |\n",
      " 0  |   6  | 313764.71875 |\n",
      " 0  |   7  | 313387.28125 |\n",
      " 0  |   8  | 313160.25000 |\n",
      " 0  |   9  | 312944.28125 |\n",
      " 0  |   10  | 312665.25000 |\n",
      " 0  |   11  | 312387.12500 |\n",
      " 0  |   12  | 312105.12500 |\n",
      " 0  |   13  | 311653.00000 |\n",
      " 0  |   14  | 311270.43750 |\n",
      " 0  |   15  | 310746.40625 |\n",
      " 0  |   16  | 310103.18750 |\n",
      " 0  |   17  | 309715.03125 |\n",
      " 0  |   18  | 309419.03125 |\n",
      " 0  |   19  | 309397.87500 |\n",
      " 0  |   20  | 309395.12500 |\n",
      " 0  |   21  | 309388.90625 |\n",
      " 0  |   22  | 309387.06250 |\n",
      " 0  |   23  | 309387.06250 |\n",
      "------------------------------  Optimizing next EM step  ------------------------------\n",
      " EM step  |   LBFGS iter  | criteria |\n",
      " 1  |   1  | 308595.78125 |\n",
      " 1  |   2  | 307733.53125 |\n",
      " 1  |   3  | 307408.71875 |\n",
      " 1  |   4  | 307199.12500 |\n",
      " 1  |   5  | 306834.90625 |\n",
      " 1  |   6  | 306507.00000 |\n",
      " 1  |   7  | 306299.53125 |\n",
      " 1  |   8  | 306207.21875 |\n",
      " 1  |   9  | 306113.37500 |\n",
      " 1  |   10  | 306033.50000 |\n",
      " 1  |   11  | 306017.06250 |\n",
      " 1  |   12  | 306010.56250 |\n",
      " 1  |   13  | 306000.93750 |\n",
      " 1  |   14  | 305985.46875 |\n",
      " 1  |   15  | 305970.84375 |\n",
      " 1  |   16  | 305967.46875 |\n",
      " 1  |   17  | 305965.40625 |\n",
      " 1  |   18  | 305964.00000 |\n",
      " 1  |   19  | 305962.34375 |\n",
      " 1  |   20  | 305961.00000 |\n",
      " 1  |   21  | 305960.25000 |\n",
      " 1  |   22  | 305959.90625 |\n",
      " 1  |   23  | 305959.59375 |\n",
      " 1  |   24  | 305959.31250 |\n",
      " 1  |   25  | 305959.18750 |\n",
      " 1  |   26  | 305959.12500 |\n",
      " 1  |   27  | 305959.12500 |\n",
      "------------------------------  Optimizing next EM step  ------------------------------\n",
      " EM step  |   LBFGS iter  | criteria |\n",
      " 2  |   1  | 305679.40625 |\n",
      " 2  |   2  | 305520.37500 |\n",
      " 2  |   3  | 305428.43750 |\n",
      " 2  |   4  | 305337.21875 |\n",
      " 2  |   5  | 305290.34375 |\n",
      " 2  |   6  | 305225.81250 |\n",
      "Objective function is NAN. Probably due to empty class\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    success, loglike = train_with_LBFGS(model)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"KeyboardInterrupt detected, stopping training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reparametrization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(   nu_a,\n",
    "    rho_a,\n",
    "    nu_b,\n",
    "    rho_b,\n",
    "    nu_p,\n",
    "    rho_p,\n",
    "    nu_q,\n",
    "    rho_q,\n",
    "    tau_1,\n",
    "    tau_2,\n",
    "    mu_un,\n",
    "    sigma_sq_a,\n",
    "    sigma_sq_b,\n",
    "    sigma_sq_p,\n",
    "    sigma_sq_q,\n",
    "    alpha_1,\n",
    "    alpha_2,\n",
    "    pi,\n",
    ") = reparametrized_expanded_params(torch.cat((model.variationnal_params, model.model_params)), n1, n2, nq, nl, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save parameters in YAML file (trained_parameters.yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_dict = {\n",
    "    'n1': n1, \n",
    "    'n2': n2,\n",
    "    'nq': nq, \n",
    "    'nl': nl,\n",
    "    'nu_a':nu_a,\n",
    "    'rho_a':rho_a,\n",
    "    'nu_b':nu_b,\n",
    "    'rho_b':rho_b,\n",
    "    'nu_p': nu_p,\n",
    "    'rho_p':rho_p,\n",
    "    'nu_q':nu_q,\n",
    "    'rho_q':rho_q,\n",
    "    'tau_1':tau_1,\n",
    "    'tau_2':tau_2,\n",
    "    'mu_un':mu_un,\n",
    "    'sigma_sq_a': sigma_sq_a,\n",
    "    'sigma_sq_b':sigma_sq_b,\n",
    "    'sigma_sq_p':sigma_sq_p,\n",
    "    'sigma_sq_q':sigma_sq_q,\n",
    "    'alpha_1':alpha_1,\n",
    "    'alpha_2':alpha_2,\n",
    "    'pi':pi,\n",
    "    'indices_p': indices_p,\n",
    "    'indices_n':indices_n,\n",
    "    'indices_zeros': indices_zeros,\n",
    "    'device': device, \n",
    "    'device2': device2,\n",
    "}\n",
    "\n",
    "save_objects_to_yaml(parameters_dict, 'trained_parameters.yaml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
